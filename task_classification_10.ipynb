{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-Mb7bnoaRLM"
   },
   "source": [
    "# Логичтическая регрессия, метод опорных векторов, one-hot кодирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNyXo3CvaRLP"
   },
   "source": [
    "### О задании\n",
    "\n",
    "В этом задании вы изучите методы работы с категориальными переменными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ndj090dOaRLQ",
    "outputId": "fdef6337-1efe-4945-d445-fb17c46dcc4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_wS0x7RaRLR"
   },
   "source": [
    "__Задание 1.__ Обучение логистической регрессии на реальных данных и оценка качества классификации.\n",
    "\n",
    "**(2 балла)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94eHa5RgaRLS"
   },
   "source": [
    "Загрузим данные с конкурса [Kaggle Porto Seguro’s Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction) (вам нужна только обучающая выборка). Задача состоит в определении водителей, которые в ближайший год воспользуются своей автомобильной страховкой (бинарная классификация). Но для нас важна будет не сама задача, а только её данные. При этом под нужды задания мы немного модифицируем датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BJQn-94DaRLS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('train.csv', index_col=0)\n",
    "target = data.target.values\n",
    "data = data.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2Su7GNhaRLT"
   },
   "source": [
    "Пересемплируем выборку так, чтобы положительных и отрицательных объектов в выборке было одинаковое число. Разделим на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fot9A7L8aRLT"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(910)\n",
    "mask_plus = np.random.choice(np.where(target == 1)[0], 100000, replace=True)\n",
    "mask_zero = np.random.choice(np.where(target == 0)[0], 100000, replace=True)\n",
    "\n",
    "data = pd.concat((data.iloc[mask_plus], data.iloc[mask_zero]))\n",
    "target = np.hstack((target[mask_plus], target[mask_zero]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GB0kVSoaRLT"
   },
   "source": [
    "Не забудьте отнормировать признаки (можно воспользоваться StandardScaler или сделать это вручную). Пока не будем обращать внимание на то, что некоторые признаки категориальные (этим мы займёмся позже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5dDctZhDaRLU"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSrjeimpaRLU"
   },
   "source": [
    "Обучите логистическую регрессию с удобными для вас параметрами, примените регуляризацию, найдтие оптимум. Сделайте предсказание на тестовой части выборки. Замерьте качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_k(y_pred, y_test):\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    precision = precision_score(y_pred, y_test)\n",
    "    recall = recall_score(y_pred, y_test)\n",
    "    f1 = f1_score(y_pred, y_test)\n",
    "    \n",
    "    print(f\"accuracy: {accuracy:.4f}\")\n",
    "    print(f\"precision: {precision:.4f}\")\n",
    "    print(f\"recall: {recall:.4f}\")\n",
    "    print(f\"f1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "A1tEHyNFaRLU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5882\n",
      "precision: 0.5486\n",
      "recall: 0.5958\n",
      "f1: 0.5712\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = LogisticRegression(C=1.0, solver='liblinear', max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print_k(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9UQ31XFaRLU"
   },
   "source": [
    "__Выводы__ в свободной форме:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Качество модели не очень высокое, однако в большинстве случаев модель классифицирует верно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTUVvmreaRLV"
   },
   "source": [
    "__Задание 2.__ Изучение влияния регуляризатора на процесс обучения\n",
    "\n",
    "__(2 балла)__\n",
    "\n",
    "Проверьте на практике, как влияет регуляризатор на процесс обучения (убывание функции потерь на обучающей и отложенной выборках). Чтобы считать функцию потерь на отложенной выборке после каждой итерации, запускайте процесс обучения логистической регрессии с параметром $max\\_iter=1$ и $w^{(0)}$, полученным на предыдущей итерации. Постройте два графика: на одном из них логистическая регрессия с коэффициентом регуляризации, равным 0, а на другом с некоторым разумным значением. На каждом графике одновременно должна быть и функция потерь для обучающей, и для тестовой выборки. Не забудьте сделать одинаковыми оси обоих графиков. Какие выводы вы можете сделать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k4YkmjpIaRLV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m         test_losses\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_losses, test_losses\n\u001b[0;32m---> 30\u001b[0m train_losses_no_reg, test_losses_no_reg \u001b[38;5;241m=\u001b[39m track_logistic_regression_loss(\n\u001b[1;32m     31\u001b[0m     X_train_scaled, X_test_scaled, y_train, y_test, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     33\u001b[0m train_losses_reg, test_losses_reg \u001b[38;5;241m=\u001b[39m track_logistic_regression_loss(\n\u001b[1;32m     34\u001b[0m     X_train_scaled, X_test_scaled, y_train, y_test, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Строим графики\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mtrack_logistic_regression_loss\u001b[0;34m(X_train, X_test, y_train, y_test, C, max_iter)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m---> 14\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Предсказания вероятностей\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     y_train_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, prefer\u001b[38;5;241m=\u001b[39mprefer)(\n\u001b[1;32m   1351\u001b[0m     path_func(\n\u001b[1;32m   1352\u001b[0m         X,\n\u001b[1;32m   1353\u001b[0m         y,\n\u001b[1;32m   1354\u001b[0m         pos_class\u001b[38;5;241m=\u001b[39mclass_,\n\u001b[1;32m   1355\u001b[0m         Cs\u001b[38;5;241m=\u001b[39m[C_],\n\u001b[1;32m   1356\u001b[0m         l1_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio,\n\u001b[1;32m   1357\u001b[0m         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[1;32m   1358\u001b[0m         tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[1;32m   1359\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   1360\u001b[0m         solver\u001b[38;5;241m=\u001b[39msolver,\n\u001b[1;32m   1361\u001b[0m         multi_class\u001b[38;5;241m=\u001b[39mmulti_class,\n\u001b[1;32m   1362\u001b[0m         max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[1;32m   1363\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[1;32m   1364\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1365\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[1;32m   1366\u001b[0m         coef\u001b[38;5;241m=\u001b[39mwarm_start_coef_,\n\u001b[1;32m   1367\u001b[0m         penalty\u001b[38;5;241m=\u001b[39mpenalty,\n\u001b[1;32m   1368\u001b[0m         max_squared_sum\u001b[38;5;241m=\u001b[39mmax_squared_sum,\n\u001b[1;32m   1369\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1370\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_, warm_start_coef_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(classes_, warm_start_coef)\n\u001b[1;32m   1373\u001b[0m )\n\u001b[1;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:451\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    447\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (C \u001b[38;5;241m*\u001b[39m sw_sum)\n\u001b[1;32m    448\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[1;32m    449\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[1;32m    450\u001b[0m ]\n\u001b[0;32m--> 451\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mminimize(\n\u001b[1;32m    452\u001b[0m     func,\n\u001b[1;32m    453\u001b[0m     w0,\n\u001b[1;32m    454\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    455\u001b[0m     jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    456\u001b[0m     args\u001b[38;5;241m=\u001b[39m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[1;32m    457\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_iter,\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m,  \u001b[38;5;66;03m# default is 20\u001b[39;00m\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m\"\u001b[39m: iprint,\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m\"\u001b[39m: tol,\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftol\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    463\u001b[0m     },\n\u001b[1;32m    464\u001b[0m )\n\u001b[1;32m    465\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    466\u001b[0m     solver,\n\u001b[1;32m    467\u001b[0m     opt_res,\n\u001b[1;32m    468\u001b[0m     max_iter,\n\u001b[1;32m    469\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    470\u001b[0m )\n\u001b[1;32m    471\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_minimize.py:738\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    735\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    736\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 738\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    739\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    741\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    742\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_lbfgsb_py.py:386\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    383\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(x0, bounds[\u001b[38;5;241m0\u001b[39m], bounds[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# _prepare_scalar_function can use bounds=None to represent no bounds\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[38;5;241m=\u001b[39mjac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    387\u001b[0m                               bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[1;32m    388\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step)\n\u001b[1;32m    390\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    392\u001b[0m nbd \u001b[38;5;241m=\u001b[39m zeros(n, np\u001b[38;5;241m.\u001b[39mint32)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_optimize.py:291\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    287\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    292\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:223\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    220\u001b[0m     finite_diff_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_linear_operator\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Initial function evaluation\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Initial gradient evaluation\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ngev \u001b[38;5;241m=\u001b[39m _wrapper_grad(\n\u001b[1;32m    227\u001b[0m     grad,\n\u001b[1;32m    228\u001b[0m     fun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_fun,\n\u001b[1;32m    229\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    230\u001b[0m     finite_diff_options\u001b[38;5;241m=\u001b[39mfinite_diff_options\n\u001b[1;32m    231\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:295\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 295\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_fun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:21\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_optimize.py:80\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_if_needed(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_optimize.py:74\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 74\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:332\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    330\u001b[0m     grad[:n_features] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m grad_pointwise \u001b[38;5;241m+\u001b[39m l2_reg_strength \u001b[38;5;241m*\u001b[39m weights\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept:\n\u001b[0;32m--> 332\u001b[0m         grad[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m grad_pointwise\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((n_classes, n_dof), dtype\u001b[38;5;241m=\u001b[39mweights\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/numpy/_core/_methods.py:51\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def track_logistic_regression_loss(X_train, X_test, y_train, y_test, C=1.0, max_iter=1000):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    model = LogisticRegression(C=C, max_iter=1, warm_start=True, \n",
    "                              random_state=42, solver='lbfgs')\n",
    "    \n",
    "    for i in range(1, max_iter + 1):\n",
    "        model.max_iter = i\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Предсказания вероятностей\n",
    "        y_train_pred_proba = model.predict_proba(X_train)\n",
    "        y_test_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        # Вычисляем потери\n",
    "        train_loss = log_loss(y_train, y_train_pred_proba)\n",
    "        test_loss = log_loss(y_test, y_test_pred_proba)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "train_losses_no_reg, test_losses_no_reg = track_logistic_regression_loss(\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, C=1.0, max_iter=1000)\n",
    "\n",
    "train_losses_reg, test_losses_reg = track_logistic_regression_loss(\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, C=0.01, max_iter=1000)\n",
    "\n",
    "\n",
    "# Строим графики\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "#  С = 0\n",
    "iterations = range(1, len(train_losses_no_reg) + 1)\n",
    "ax1.plot(iterations, train_losses_no_reg, label='Обучающая выборка', linewidth=2)\n",
    "ax1.plot(iterations, test_losses_no_reg, label='Тестовая выборка', linewidth=2)\n",
    "ax1.set_title('Без регуляризации (C=1.0)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Итерация')\n",
    "ax1.set_ylabel('Log Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.0)\n",
    "\n",
    "С = 0.01\n",
    "ax2.plot(iterations, train_losses_reg, label='Обучающая выборка', linewidth=2)\n",
    "ax2.plot(iterations, test_losses_reg, label='Тестовая выборка', linewidth=2)\n",
    "ax2.set_title('С регуляризацией (C=0.01)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Итерация')\n",
    "ax2.set_ylabel('Log Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP-lz7NxaRLV"
   },
   "source": [
    "__Выводы:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHvrjAQnaRLW"
   },
   "source": [
    "## Часть 2. Работа с категориальными переменными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_doKeEKxaRLW"
   },
   "source": [
    "В этой части мы научимся обрабатывать категориальные переменные, так как закодировать их в виде чисел недостаточно (это задаёт некоторый порядок, которого на категориальных переменных может и не быть). Существует два основных способа обработки категориальных значений:\n",
    "- One-hot-кодирование\n",
    "- Счётчики (CTR, mean-target кодирование, ...) — каждый категориальный признак заменяется на среднее значение целевой переменной по всем объектам, имеющим одинаковое значение в этом признаке.\n",
    "\n",
    "Начнём с one-hot-кодирования. Допустим наш категориальный признак $f_j(x)$ принимает значения из множества $C=\\{c_1, \\dots, c_m\\}$. Заменим его на $m$ бинарных признаков $b_1(x), \\dots, b_m(x)$, каждый из которых является индикатором одного из возможных категориальных значений:\n",
    "$$\n",
    "b_i(x) = [f_j(x) = c_i]\n",
    "$$\n",
    "\n",
    "__Задание 1.__ Закодируйте все категориальные признаки с помощью one-hot-кодирования. Обучите логистическую регрессию и посмотрите, как изменилось качество модели (с тем, что было ранее). Измерьте время, потребовавшееся на обучение модели.\n",
    "\n",
    "__(3 балла)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GC4tPzPbaRLW"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "\n",
    "\n",
    "data = pd.read_csv('train.csv', index_col=0)\n",
    "target = data.target.values\n",
    "data = data.drop('target', axis=1)\n",
    "\n",
    "np.random.seed(910)\n",
    "mask_plus = np.random.choice(np.where(target == 1)[0], 100000, replace=True)\n",
    "mask_zero = np.random.choice(np.where(target == 0)[0], 100000, replace=True)\n",
    "\n",
    "data = pd.concat((data.iloc[mask_plus], data.iloc[mask_zero]))\n",
    "target = np.hstack((target[mask_plus], target[mask_zero]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обучения модели: 16.6489 секунд\n",
      "\n",
      "Модель c one-hot:\n",
      "accuracy: 0.5945\n",
      "precision: 0.5623\n",
      "recall: 0.6011\n",
      "f1: 0.5810\n",
      "\n",
      "Модели без one-hot:\n",
      "accuracy: 0.5882\n",
      "precision: 0.5486\n",
      "recall: 0.5958\n",
      "f1: 0.5712\n"
     ]
    }
   ],
   "source": [
    "cat_features = [col for col in X_train.columns if 'cat' in col]\n",
    "num_features = [col for col in X_train.columns if col not in cat_features]\n",
    "\n",
    "onehot_encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "X_train_cat_encoded = onehot_encoder.fit_transform(X_train[cat_features])\n",
    "X_test_cat_encoded = onehot_encoder.transform(X_test[cat_features])\n",
    "\n",
    "encoded_feature_names = onehot_encoder.get_feature_names_out(cat_features)\n",
    "\n",
    "X_train_encoded_df = pd.DataFrame(\n",
    "    X_train_cat_encoded, \n",
    "    columns=encoded_feature_names, \n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_encoded_df = pd.DataFrame(\n",
    "    X_test_cat_encoded, \n",
    "    columns=encoded_feature_names, \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "\n",
    "X_train_combined = pd.concat([X_train[num_features], X_train_encoded_df], axis=1)\n",
    "X_test_combined = pd.concat([X_test[num_features], X_test_encoded_df], axis=1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "onehot_model = LogisticRegression(random_state=42, max_iter=2000, n_jobs=-1)\n",
    "onehot_model.fit(X_train_combined, y_train)\n",
    "\n",
    "onehot_train_time = time.time() - start\n",
    "\n",
    "\n",
    "y_pred_onehot = onehot_model.predict(X_test_combined)\n",
    "\n",
    "\n",
    "print(f\"Время обучения модели: {onehot_train_time:.4f} секунд\")\n",
    "print(\"\\nМодель c one-hot:\") \n",
    "print_k(y_pred_onehot, y_test)\n",
    "print(\"\\nМодели без one-hot:\")\n",
    "print_k(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izITXcOWaRLW"
   },
   "source": [
    "Как можно было заменить, one-hot-кодирование может сильно увеличивать количество признаков в датасете, что сказывается на памяти, особенно, если некоторый признак имеет большое количество значений. Эту проблему решает другой способ кодирование категориальных признаков — счётчики. Основная идея в том, что нам важны не сами категории, а значения целевой переменной, которые имеют объекты этой категории. Каждый категориальный признак мы заменим средним значением целевой переменной по всем объектам этой же категории:\n",
    "$$\n",
    "g_j(x, X) = \\frac{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)][y_i = +1]}{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)]}\n",
    "$$\n",
    "\n",
    "__Задание 2.__ Закодируйте категориальные переменные с помощью счётчиков (ровно так, как описано выше без каких-либо хитростей). Обучите логистическую регрессию и посмотрите на качество модели на тестовом множестве. Сравните время обучения с предыдущим экспериментов. Заметили ли вы что-то интересное?\n",
    "\n",
    "__(2 балла)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJmhJjcyaRLW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ6BybtVaRLW"
   },
   "source": [
    "__Вывод:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUSCGlbhaRLW"
   },
   "source": [
    "Отметим, что такие признаки сами по себе являются классификаторами и, обучаясь на них, мы допускаем \"утечку\" целевой переменной в признаки. Это ведёт к переобучению, поэтому считать такие признаки необходимо таким образом, чтобы при вычислении для конкретного объекта его целевая метка не использовалась. Это можно делать следующими способами:\n",
    "- вычислять значение счётчика по всем объектам расположенным выше в датасете (например, если у нас выборка отсортирована по времени)\n",
    "- вычислять по фолдам, то есть делить выборку на некоторое количество частей и подсчитывать значение признаков по всем фолдам кроме текущего (как делается в кросс-валидации)\n",
    "- внесение некоторого шума в посчитанные признаки (необходимо соблюсти баланс между избавление от переобучения и полезностью признаков).\n",
    "\n",
    "__Задание 3.__ Реализуйте корректное вычисление счётчиков двумя из трех вышеперчисленных способов, сравните. Снова обучите логистическую регрессию, оцените качество. Сделайте выводы.\n",
    "\n",
    "__(3 балла)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX9gBIEJaRLW"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIXbvzlWaRLX"
   },
   "source": [
    "__Вывод:__"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
